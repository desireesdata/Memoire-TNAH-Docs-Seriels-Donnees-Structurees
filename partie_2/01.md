# Une histoire par les données

Les *Tables Annuelles* du Sénat, comme on vient de le voir, contiennent une véritable mine d'informations pour établir une analyse de l'activité parlementaire. Ces *Tables*, accessibles sur Gallica, avec le jeu des renvois et des index, sont de véritables bases de données de papier numérisées. Pour récupérer les informations du *Journal Officiel* de façon automatisée, c'est-à-dire sans reproduire à la main l'ensemble, il faut penser à une chaîne de traitement qui part de ces sources numériques, sous format image, pour pouvoir en capturer l'information. Il s'agit ici de voir comment construire un protocole d’extraction cohérent, en tenant compte de la matérialité des documents eux-mêmes, sous leur forme "analogique"; mais aussi sous leur forme "numérique". Dans ce chapitre, il s'agira de répondre aux problématiques technique de cette traduction des sources numérisées -- c'est-à-dire sous format image -- au texte. Ceci imposant de donner un contexte préalable de cette "mise en données" des sources historiques, laquelle est inhérente à la disponibilité de corpus numérisés par les politiques de valorisation des fonds des institutions patrimoniales. [Section 1: Datafication des corpus : "numériser"]

Ensuite, premier problème : comment travailler à partir d'une image numérique ? Certes, la représentation photographique et numérique d'un document est lisible pour un oeil humain; mais du point de vue informationnel, ces images ne sont que des paquets de pixels. Ces pixels ne sont pas, évidemment, les lettres elles-mêmes. Ils sont la traduction sur l'écran de trains d'informations binaires qui, sans le bon décodage, pourrait vouloir dire tout autre chose. Le premier enjeu pour un travail de capture de l'information est de transformer cette matière matricielle en information textuelle sur laquelle on peut appliquer des traitements. Le texte se présente comme pré-requis pour établir des chaînes de traitement de capture informationnelle. Ce passage de l'image au texte numérique est en fait techniquement une prérogative des tâches de *reconnaissance optique des caractères* -- ou "OCR" (Optical Character Recognition). Elle butte également sur des problématiques de détection de la mise en page, laquelle fonde un ordre de lecture -- et donc un agencement du sens des phrases qu'il faut considérer. [Section 2 : de l'image au texte]

Deuxième problème : une fois ce texte numérique obtenu, comment capturer l'information sémantique qui est présente ? Comment l'ordinateur peut comprendre que tel ensemble des caractères alphanumériques correspond en fait à un sénateur de la Troisième République ? On peut trouver, dans le document, des motifs qui signalent une entité (par exemple, un sénateur inaugure chaque paragraphe). Cette approche comme on va le voir, est basée sur la reconnaissance de motifs typographiques. Elle est cependant fragile et dépendante de la qualité de l'OCR -- voire des erreurs humaines présentes dans le document d'origine. Elle suppose aussi une forme de connaissance *a priori* synthétique de la représentation de l'information dans le document. Ainsi peut-on se tourner vers des approche extractive (les Bert) ou bien les approches génératives qui "lisent" le texte et restitue l'information comprise et permettent de contourner le problème des exceptions qui forment le corps des documents. Cette chaîne de travail de capture est tourné vers un format de cette information exploitable par l'ordinateur. La chaîne de traitement commence donc avec l'image, passe par le texte et des méthodes de capture de l'information sémantique qu'elle contient, pour aboutir à une information structurée. L'enjeu n'est pas simple car chaque étape reporte les marges d'erreur des précédentes. [Section 3]

Dans ce chapitre, il s'agira ainsi de dessiner le contexte technique et institutionnel de cette datafication des données en vue de leur traitement -- et notamment avec les nouvelles opportunités des grands modèles de langage.

## Datafication des corpus : *numériser*

### En "mode texte"

En 1971, un étudiant, reproduisait sur un ordinateur *Xerox* la *Déclaration d'indépendance des Etats-Unis*, en caractères alphanumériques **ASCII**. Il s'agissait de Michel Hart, fondateur du **Projet Gutenberg** qui se donnait pour tâche de reproduire et diffuser bénévolement sur le réseau internet des oeuvres littéraires du domaine public. La Bible, les oeuvres de Shakespeare, quelques autres de Lewis Carroll ou de James M. Barrie seront notamment reproduites. Ce travail de "numérisation" est en fait un travail laborieux : chacune des lettres de chaque livre sera tapée à la main, les unes après les autres. En 1990, de façon contemporaine à la jeunesse du Web, le projet prend un nouvel essor et bénéficie d'une collaboration internationale : les collections s'élèvent à environ 1000 livres en 1997; 4000 livres en 2001; et $15000$ livres en 2005 [Marie Lebert]. Entre le livre et la version numérique, il n'y a pas d'image : juste le travail de transcription manuel des caractères. C'est une numérisation des livres "en mode texte" [Bermès, 30-33] l'information textuelle seule est reproduite, cela destructurant l'objet livre. Avec cette reproduction en caractères alphanumériques, la structure physique du livre -- sa mise en page -- est perdue; mais on peut en revanche rechercher un mot et retrouver un passage plus aisément. 

Le texte numérique ne se définit pas seulement comme une reproduction électronique du texte imprimé, mais comme une transformation de l’information en une suite de signes codés. Concrètement, chaque caractère est représenté par une valeur numérique, selon un système de codage -- ainsi tel que l’**ASCII** (American Standard Code for Information Interchange) ou, plus récemment, l’**Unicode**, qui attribue à chaque lettre, chiffre ou symbole une séquence binaire, une suite de *bits*, c’est-à-dire de 0 et de 1. Ce passage de l’écriture alphabétique à la codification binaire permet au texte d’être manipulé comme une donnée discrète : il devient possible de rechercher automatiquement un mot, de compter des occurrences, de structurer des chaînes de caractères.

Cette démarche d'encodage de l'information, qui ne concerne pas ici proprement l’historien, est exemplaire au regard des méthodes de *numérisation* des documents textuels en ce sens qu’elle traduit une forme analogique — physique ou continue — en une forme numérique, discrète. L’opération de transcription manuelle, caractère par caractère, est ici comparable à celle d’un dépouillement systématique sur archives papier : il s’agit de saisir l’information contenue dans les sources dans un dispositif tabulaire, par exemple un tableur [Claire Lemercier, Claire Zalc]. La similarité n’est toutefois que d’ordre opératoire. Dans le cas de Michel Hart et du Projet Gutenberg, la répétition du texte littéraire reste relativement linéaire et vise une reproduction intégrale, sans problématisation des sources. À l’inverse, la transcription historienne suppose une enquête critique : sélectionner, structurer, et souvent synthétiser des données pour les « mettre en table », c’est-à-dire les rendre comparables et cumulables. Cette dimension opératoire peut être qualifiée, avec Simondon, de travail de *transduction technique* : un processus par lequel l’information passe d’un support et d’un régime de signification à un autre, selon des contraintes à la fois matérielles et intellectuelles. Dans le cas des historiens, ce passage implique un véritable travail d’individuation des données : découper des flux documentaires continus en unités discrètes (noms, dates, professions, événements), qui ne préexistent pas à l’opération de transcription mais sont construites par elle. La numérisation est une opération configuratrice où la saisie manuelle se fait l’instrument d’un changement de régime technique du support de l'information. Du côté des historiens, ce travail de transduction informationnelle, plus sophistiqué que la transcription littéraire, peut être comparé au travail de terrain du sociologue ou de l'éthnographe, en ce sens qu'elle suscite justement des questions et reconfigure les valuations de l'enquête [Dewey; Claire Lemercier, Claire Zalc].

Le travail de saisie manuelle n’est évidemment pas une nouveauté introduite par l’ordinateur. Bien avant l’ère numérique, les historiens s’y adonnaient déjà. Ainsi, à la fin des années 1940, Pierre Chaunu recopiait à la main, sur papier, les données issues des archives et des ouvrages nécessaires à sa thèse, afin de les ordonner et de les exploiter systématiquement [Bertrand Müller]. Aujourd’hui encore, malgré l’apparition d’outils de transcription automatique [voir section 2], cette pratique demeure courante : toutes les sources ne sont pas disponibles en version numérique, et le chercheur, tout comme l’étudiant ou le généalogiste, peut être amené à relever lui-même les informations qui l’intéressent, directement en salle d’archives ou lors du dépouillement de fonds imprimés.

Le mode opératoire de la saisie manuelle constitue en ce sens un exemple singulier, puisqu’il s’oppose radicalement à la logique de la numérisation photographique, dite « en mode image » [Bermès]. Cette opposition met en évidence deux conceptions distinctes de la numérisation : d’un côté, la transcription textuelle, qui construit les données par un travail de sélection et de structuration ; de l’autre, la reproduction visuelle, qui se limite à conserver la matérialité de l’objet. L’histoire des pratiques documentaires, tant individuelles qu’institutionnelles, témoigne de cette tension durable entre deux paradigmes concurrents [Bermès, 29].

### En "mode image"

À l’opposé du « mode texte », la numérisation en "mode image" repose sur la reproduction photographique des documents, cherchant à restituer leur matérialité visuelle : texte, blancs, marges, typographie, ornements, etc. Tout est fixé dans une matrice de pixels. Héritière des microfilms et des fac-similés, cette pratique connaît une expansion décisive dans les années 1990, avec l’essor du Web et le lancement des premières grandes campagnes institutionnelles de numérisation. Deux projets emblématiques illustrent cette dynamique : Gallica (BnF, 1997) et Google Books (2004). Leur ambition est similaire — mettre à disposition le patrimoine imprimé à grande échelle — mais leur logique diverge : Gallica s’inscrit dans une mission de service public, garantissant fidélité documentaire, conservation et interopérabilité (Bermès, *Trente ans de numérique à la BnF*, 2023), tandis que Google privilégie la puissance de l’indexation et la recherche plein texte, au détriment de l’intégrité patrimoniale des objets.

La numérisation en mode image est avant tout une démarche institutionnelle. Elle suppose des investissements lourds en infrastructures, en personnels et en politiques documentaires. Elle se distingue ainsi des pratiques de transcription textuelle, souvent issues de gestes individuels ou collaboratifs (historien recopiant ses sources, dépouillements collectifs, corrections d’OCR par crowdsourcing). Certes, des chercheurs ou amateurs produisent eux aussi des photographies de documents — parfois propres, parfois « à l’arrache » —, mais ces fichiers restent isolés, de qualité variable, sans métadonnées ni garantie de pérennité. Ils ne deviennent pas des corpus mais des archives personnelles. Les institutions, au contraire, inscrivent leurs campagnes dans des stratégies de patrimonialisation, produisant des ensembles cohérents et diffusables à large échelle.

Sur le plan technique, ces images sont matricielles : elles fixent l’apparence de la page mais le contenu intellectuel "littéraire" demeure opaques pour l’ordinateur. Qu’elles proviennent d’une campagne institutionnelle ou d’un smartphone amateur, elles ne permettent pas la recherche plein texte sans OCR ou segmentation. Dans le cas des photographies amateurs, souvent floues ou mal cadrées, l’OCR est même impraticable, réduisant ces fichiers à des fac-similés inertes, reportant ainsi la tâche de saisie manuelle non pas à partir des documents matériels, mais à partir de leur représentation numérique.

Il importe ainsi de souligner que numériser n’est pas éditer. Là où la transcription ou la mise en table impliquent un acte de curation — sélection, structuration, annotation —, la capture photographique ne fournit qu’une matière brute. Sans enrichissement éditorial, notamment par des métadonnées ou par l’application d’OCR, ces images restent orphelines de leur information textuelle, simples objets visuels sans accès direct à leur contenu. Leur valeur scientifique dépend donc entièrement des traitements ultérieurs qui les convertissent en données exploitables.

En ce sens, l’histoire des pratiques documentaires révèle une tension durable entre deux régimes : tout d'abord la transcription textuelle, enracinée dans des pratiques individuelles ou collaboratives, qui construisent l’information par sélection et structuration ; ensuite, la reproduction visuelle, portée par des politiques institutionnelles, qui produit des corpus massifs mais souvent réduits à l’état de matière brute.

Cette opposition doit enfin être replacée dans l’horizon plus large de la **datafication** (Clavert, Rygiel) : numériser, ce n’est pas seulement reproduire, c’est transformer des artefacts en données, dans un cadre technique, social et institutionnel qui oriente les usages, fixe les normes de conservation et conditionne l’accès même aux sources.

### Réseaux et datafication : sphère technique, sphère sociale

La *datafication* est le processus qui vise à quantifier un phénomène de sorte qu'il soit calculable et analysable [Frédéric Clavert]. Elle considère le passage du continu au discret; la calculabité des phénomènes comme une prérogative du "numérique". Cette mise en données "insiste sur la notion de processus" et "se définit par les choix opérés par les organismes qui y procèdent", cela impliquant "les critères d'inclusion [de] corpus à numériser"; l'élaboration de métadonnées descriptives situées, lesquelles ont un impact sur leur découvrabilité puisque les moteurs de recherche s'y appuient [Frédéric Clavert, 123].

La datafication ne consiste pas simplement à transformer un document papier en fichier numérique : c’est un processus complexe qui combine numérisation, structuration et mise en base de données. En histoire, cela signifie passer du papier (via OCR ou HTR) à un texte exploitable, puis à des entités ou métadonnées normalisées (XML-TEI, bases relationnelles). Ce processus, qui permet de rendre les corpus calculables et interopérables, ouvre certes des perspectives considérables — recherche plein texte, analyses sérielles, croisement de sources — mais il engage aussi une série de choix méthodologiques et institutionnels qui, étant donné leur lourdeur produisent leurs propres biais.

Ces choix se jouent d’abord dans la sélection des corpus à numériser : les bibliothèques privilégient souvent les imprimés homogènes ou les fonds les plus demandés, laissant de côté des séries manuscrites plus fragiles ou moins visibles. Comme l’a montré le projet TIME-US, cette logique favorise la présence de sources officielles ou institutionnelles (journaux ouvriers imprimés, presse syndicale) et relègue dans l’ombre des archives plus marginales (pétitions manuscrites, traces de travail informel). La datafication amplifie ainsi certains silences archivistiques : ce qui n’a pas été consigné, ou ce qui est difficile à transcrire automatiquement, reste hors champ.

S’y ajoutent les biais techniques. L’OCR ou la HTR sont rarement neutres : leur performance varie selon l’état du document, la langue, l’alphabet ou la typographie. Dans TIME-US, il a fallu corriger manuellement des milliers d’occurrences et entraîner des modèles spécifiques pour le français pré-moderne. Par ailleurs, la normalisation des données (dates, métiers, entités) tend à gommer des variations significatives et risque de projeter des catégories anachroniques. De ce point de vue, la datafication ne produit pas des « données brutes », mais bien des données construites, filtrées par des choix techniques.

Enfin, la datafication est aussi un processus social : elle reflète et prolonge les hiérarchies documentaires héritées. Les corpus numérisés surreprésentent souvent les groupes dominants (élites, institutions, employeurs), au détriment des voix minoritaires. Le danger est alors de tomber dans un effet d’« Eldorado numérique » : l’historien travaille sur ce qui est disponible, non sur ce qui est historiquement pertinent. La question devient dès lors : comment documenter ces biais et construire la confiance dans des données issues de systèmes techniques ?

biais des choix institutionnels

Jean-Marie Dallet, Drucker

### Un standard à la croisée de la sphère technique et la sphère sociale : IIIF

C’est dans ce contexte qu’émergent des standards comme le IIIF (International Image Interoperability Framework), né dans les années 2010 sous l’impulsion d’un consortium de grandes bibliothèques (BnF, British Library, Stanford, etc.). IIIF répond à une double exigence, à savoir : uniformiser l’accès aux images numérisées en définissant des API permettant de zoomer, annoter, partager et intégrer les images dans des environnements divers et favoriser l’interopérabilité entre institutions en permettant qu’un même document numérisé à Londres, Paris ou New York puisse être consulté, manipulé et enrichi dans une interface commune.

IIIF illustre bien l’évolution de la numérisation vers la *datafication* : il ne s’agit plus seulement de stocker et montrer des images, mais de les intégrer dans un écosystème où elles deviennent manipulables, annotables, recombinables. Un manuscrit, une affiche ou un numéro du *Journal Officiel* numérisé n’est plus seulement une image : c’est une ressource ouverte, potentiellement enrichie par des métadonnées, des annotations collaboratives ou des algorithmes d’analyse.

### "Le goût de l'archive à l'ère numérique"

La réflexion sur la datafication des corpus ne peut être dissociée du « goût de l’archive » à l’ère numérique. Le terme, mobilisé par le collectif [retrouver les noms exacts], met en évidence un paradoxe : alors que la numérisation promet une accessibilité inédite aux archives, elle en modifie profondément l’expérience sensible. L’historien ne se trouve plus confronté à des boîtes, des liasses ou des volumes, mais à des corpus massifs, fragmentés, médiés par des interfaces, des moteurs de recherche ou des API.

Ce déplacement fait écho à un débat ancien dans l’historiographie française. Dès les années 1960, Chaunu ou Le Roy Ladurie, dans le sillage de l’École des Annales, avaient déjà revendiqué l’importance des archives sérielles — registres de baptêmes, testaments, cadastres, minutes notariales — au détriment des sources narratives ou « événementielles » jugées plus séduisantes. Ils affirmaient que l’histoire pouvait (et devait) se nourrir de ces « archives grises », répétitives, sans attrait esthétique ni émotionnel, mais capables, une fois cumulées et quantifiées, de révéler des structures profondes (démographiques, sociales, économiques). Autrement dit, l’absence de « goût » de ces archives constituait paradoxalement leur force heuristique.

L’ère numérique radicalise ce basculement : ce qui, dans les années 1970, nécessitait des dépouillements manuels interminables et le recours à l’informatique balbutiante, peut désormais être automatisé et amplifié à une échelle inédite. La datafication prolonge l’intuition des Annales en rendant ces fonds sériels interrogeables et manipulables à grande vitesse. Mais elle modifie aussi l’expérience sensible : le « goût » ne réside plus dans l’objet matériel de l’archive, mais dans la découverte de motifs et de régularités rendus visibles par des visualisations, des bases de données ou des modèles.

Ainsi, l’historien se trouve à nouveau confronté à un paradoxe : les archives sérielles, longtemps délaissées pour leur monotonie, acquièrent une nouvelle attractivité dans l’espace numérique, mais au prix d’un changement de régime du sensible. Le risque est alors de réduire l’archive à son seul potentiel calculable. Le défi, aujourd’hui comme hier, est de concilier la puissance cumulative de ces données avec la vigilance critique nécessaire à l’interprétation de leurs conditions de production.

## De l'image au texte : la reconnaissance optique de caractère (OCR)

Paradigme de l'image numérisée  ==> OCR. Présentation de la dimension technique de l'image numérique matricielle. Information riche et compliquée. C'est le cas de Gallica et spécifiquement du corpus du JO. Problématique de l'extraction repose sur des stratégies d'obtention du texte, à partir duquel on pourra effectuer des traitements extracttifs

Comme on l'a vu dans le chapitre précédent, les *Tables Annuelles* du Sénat sont disponibles sur Gallica. D'un point de vue technique, ces *Tables* sont des documents numérisés, c'est-à-dire des images dont on aura discrétisé l'information. Une image numérique est un tableau -- une *matrice* -- d'une largeur et d'une hauteur données, comportant alors $largeur \times hauteur$ pixels, pixels qui encode l'information colorimétrique sur trois vecteurs : le paramètre $rouge$, le paramètre $vert$, et le paramètre $bleu$. La combinaison de ces trois paramètres, selon les règles de la synthèse colorimétrique additive, permettent de restituer, pour chaque pixel, l'ensemble des couleurs du spectre visible.

## Données brutes, données structurées : quelques enjeux de l'interopérabilité.

Le format .txt, JSON, CSV, XSLX. Envisager

# Du texte à la donnée structurée : capturer la sémantique

## Approche à motifs explicites : les ReGex

Une première approche naïve d'extraction de l'information du texte : les regex. Puissants, rapides. Mais rigide et implique de connaître à l'avance la forme de ce qu'on cherche, ce qui n'est pas trivial ! Il faut aussi partir du principe que l'on a pas une connaissance synthétique a priori de l'information. Il y a toujours un "hic". Fragile face au bruit ocr, aux fautes typographiques inattendues; et avoir une regex plus souple, c'est aussi prendre le risque de capter du bruit.

> La recherche floue Un moyen de diluer la rigidité des motifs; mais ne permet que de trouver ce que l'on connaît à l'avance. Dans un optique d'extraction massive, on veut tout sortir automatiquement.
> 
> > Automates finis !

La contrainte forte des regex Intéressant à coupler avec d'autres approches plus souple comme on le verra.

- **Patrons linguistiques** (grammaires, dépendances syntaxiques)

- **Listes de référence / gazetteers**

- **Règles de post-traitement**  
  => Avantage : explicable, prévisible  
  => Limite : peu robustes aux variations inattendues

## Approches extractives : l'approche Bert (one-to-one)

L'approche du surlignage 1 to 1.

**Principe** : le modèle apprend à repérer les entités dans un texte via des annotations.

- **Modèles supervisés classiques** : CRF, SVM, MaxEnt

- **Neuraux séquentiels** : BiLSTM-CRF, CNN-LSTM

- **Transformers extractifs** : BERT, RoBERTa, CamemBERT en mode NER  
  => Avantage : généralise mieux, bonne précision  
  => Limite : nécessite des données annotées et un entraînement

## Approches génératives : les LLMs

**Principe** : le modèle produit directement le résultat structuré à partir du texte, sur la base d’une consigne en langage naturel.

- **LLMs** (GPT, Claude, Mistral) en extraction via prompt

- **Fine-tuning génératif** (T5, GPT-4 en mode extraction JSON)  
  => Avantage : très flexible, pas besoin de jeu d’entraînement spécialisé  
  => Limite : variabilité, hallucinations, besoin de validation

## Approches hybrides

**Principe** : combiner plusieurs catégories dans un flux de traitement.

- Exemple : Gazetteer pour repérer des entités connues + BERT pour les autres + Regex pour les formats normés + validation humaine  
  => Avantage : maximiser précision et rappel  
  => Limite : complexité d’intégration

# Données et FAIRness : de la valuation et de l'évaluation

La dimension "personnelle" des données, la FAIRNESS ==> implique d'expliciter la "sitaution" des données, de leurs valuations et de leur qualité (évaluation. Nécessité d'évaluer. 

« Most literary scholars would no more simply use the “results” *of a fellow scholar than they would use her toothbrush* » (Responses to Moretti, p. 4). 5
