# Une histoire par les données

Les *Tables Annuelles* du Sénat, comme on vient de le voir, contiennent une véritable mine d'informations pour établir une analyse de l'activité parlementaire. Ces *Tables*, accessibles sur Gallica, avec le jeu des renvois et des index, sont de véritables bases de données de papier numérisées. Pour récupérer les informations du *Journal Officiel* de façon automatisée, c'est-à-dire sans reproduire à la main l'ensemble, il faut penser à une chaîne de traitement qui part de ces sources numériques, sous format image, pour pouvoir en capturer l'information. Il s'agit ici de voir comment construire un protocole d’extraction cohérent, en tenant compte de la matérialité des documents eux-mêmes, aussi bien sous leur forme "analogique" que numérique. 

Dans ce chapitre, il s'agira de répondre aux problématiques technique de cette traduction des sources numérisées -- c'est-à-dire sous format image -- au texte. Ceci imposant de donner un contexte préalable de cette "mise en données" des sources historiques, laquelle est inhérente à la disponibilité de corpus numérisés par les politiques de valorisation des fonds des institutions patrimoniales. [Section 1: Datafication des corpus : "numériser"]

Ensuite, premier problème : comment travailler à partir d'une image numérique ? Certes, la représentation photographique et numérique d'un document est lisible pour un oeil humain; mais du point de vue informationnel, ces images ne sont que des paquets de pixels. Ces pixels ne sont pas, évidemment, les lettres elles-mêmes. Ils sont la traduction sur l'écran de trains d'informations binaires qui, sans le bon décodage, pourrait vouloir dire tout autre chose. Le premier enjeu pour un travail de capture de l'information est de transformer cette matière matricielle en information textuelle sur laquelle on peut appliquer des traitements. Le texte se présente comme pré-requis pour établir des chaînes de traitement de capture informationnelle. Ce passage de l'image au texte numérique est en fait techniquement une prérogative des tâches de *reconnaissance optique des caractères* -- ou "OCR" (Optical Character Recognition). Elle butte également sur des problématiques de détection de la mise en page, laquelle fonde un ordre de lecture -- et donc un agencement du sens des phrases qu'il faut considérer. [Section 2 : de l'image au texte]

Deuxième problème : une fois ce texte numérique obtenu, comment capturer l'information sémantique qui est présente ? Comment l'ordinateur peut comprendre que tel ensemble des caractères alphanumériques correspond en fait à un sénateur de la Troisième République ? On peut trouver, dans le document, des motifs qui signalent une entité (par exemple, un sénateur inaugure chaque paragraphe). Cette approche comme on va le voir, est basée sur la reconnaissance de motifs typographiques. Elle est cependant fragile et dépendante de la qualité de l'OCR -- voire des erreurs humaines présentes dans le document d'origine. Elle suppose aussi une forme de connaissance *a priori* synthétique de la représentation de l'information dans le document. Ainsi peut-on se tourner vers des approche extractive (les Bert) ou bien les approches génératives qui "lisent" le texte et restitue l'information comprise et permettent de contourner le problème des exceptions qui forment le corps des documents. Cette chaîne de travail de capture est tourné vers un format de cette information exploitable par l'ordinateur. La chaîne de traitement commence donc avec l'image, passe par le texte et des méthodes de capture de l'information sémantique qu'elle contient, pour aboutir à une information structurée. L'enjeu n'est pas simple car chaque étape reporte les marges d'erreur des précédentes. [Section 3]

Dans ce chapitre, il s'agira ainsi de dessiner le contexte technique et institutionnel de cette datafication des données en vue de leur traitement -- et notamment avec les nouvelles opportunités des grands modèles de langage.

## Datafication des corpus : *numériser*

### En "mode texte"

En 1971, un étudiant, reproduisait sur un ordinateur *Xerox* la *Déclaration d'indépendance des Etats-Unis*, en caractères alphanumériques **ASCII**. Il s'agissait de Michel Hart, fondateur du **Projet Gutenberg** qui se donnait pour tâche de reproduire et diffuser bénévolement sur le réseau internet des oeuvres littéraires du domaine public. La Bible, les oeuvres de Shakespeare, quelques autres de Lewis Carroll ou de James M. Barrie seront notamment reproduites [Marie Lebert]. Ce travail de "numérisation" est en fait un travail laborieux : chacune des lettres de chaque livre sera tapée à la main, les unes après les autres. En 1990, de façon contemporaine à la jeunesse du Web, le projet prend un nouvel essor et bénéficie d'une collaboration internationale : les collections s'élèvent à environ 1000 livres en 1997; 4000 livres en 2001; et $15000$ livres en 2005 [Marie Lebert]. Entre le livre et la version numérique, il n'y a pas d'image : juste le travail de transcription manuel des caractères. C'est une numérisation des livres "en mode texte" [Bermès, 30-33] : l'information textuelle seule, stockée sur disque dur, est reproduite sur l'écran, cela "destructurant l'objet livre" [Bermès]. Avec cette reproduction en caractères alphanumériques, la structure physique du livre -- sa mise en page -- est perdue; mais on peut en revanche rechercher un mot et retrouver un passage plus aisément. 

Le texte numérique ne se définit pas seulement comme une reproduction électronique du texte imprimé, mais comme une transformation de l’information en une suite de signes codés. Concrètement, chaque caractère est représenté par une valeur numérique, selon un système de codage -- ainsi tel que l’**ASCII** (American Standard Code for Information Interchange) ou, plus récemment, l’**Unicode**, qui attribue à chaque lettre, chiffre ou symbole une séquence binaire, une suite de *bits*, c’est-à-dire de 0 et de 1. Ce passage de l’écriture alphabétique à la codification binaire permet au texte d’être manipulé comme une donnée discrète : il devient possible de rechercher automatiquement un mot, de compter des occurrences, de structurer des chaînes de caractères.

Cette démarche d'encodage de l'information, qui ne concerne pas ici proprement l’historien, est exemplaire au regard des méthodes de *numérisation* des documents textuels en ce sens qu’elle traduit une forme analogique — physique ou continue — en une forme numérique, discrète. L’opération de transcription manuelle, caractère par caractère, est ici comparable à celle d’un dépouillement systématique sur archives papier : il s’agit de saisir l’information contenue dans les sources dans un dispositif tabulaire, par exemple un tableur [Claire Lemercier, Claire Zalc]. La similarité n’est toutefois que d’ordre opératoire. Il y a un face-à-face entre l'opérateur et la source à restranscrire. Dans le cas de Michel Hart et du Projet Gutenberg, la répétition du texte littéraire reste relativement linéaire et vise une reproduction intégrale. À l’inverse, la transcription historienne suppose une enquête critique : sélectionner, structurer, et souvent synthétiser des données pour les « mettre en table », c’est-à-dire les rendre comparables et cumulables. Cet schème opératoire transcriptif peut être qualifié, avec Simondon, de travail de *transduction technique* : un processus par lequel l’information passe d’un support et d’un régime de signification à un autre, selon des contraintes à la fois matérielles et intellectuelles. L'information change de milieu et, en contexte numérique, son inscription *in silicium*, permet de reconsidérer le texte discrétisé comme un ensemble d'éléments manipulables. Le texte numérique peut être alors considéré selon différents degrés de structuration : tantôt comme une "répétition linéaire" et brute des sources originales à l'instar des premières éditions du **Projet Gutenberg**; tantôt comme une information choisie et hiérarchisée, comme l'implique une mise en tableau.

Dans le cas des historiens, ce passage implique un véritable travail d’individuation des données -- c'est-à-dire de leur transformation au regard du contexte technique qui joue ici comme un milieu : il faut découper des flux documentaires continus en unités discrètes (noms, dates, professions, événements par exemple), qui ne préexistent pas à l’opération de transcription mais sont construites, reformulée par elle, avec la contrainte ultérieure de pouvoir retrouver l'information encodée. Des stratégies d'habillage de l'information saisie sont à envisager du même mouvement, par exemple en dotant le texte d'un "apparat critique" qui permettent un retour au texte original à travers des clés qui en indexe le contenu [Marc Van Campenhoudt]. La numérisation en mode texte est alors une opération configuratrice : la saisie manuelle se fait l’instrument d’un changement de régime technique du support de l'information car elle implique, à différent degrés, un besoin de mise en structure. Du côté des historiens, si ce travail de transduction informationnelle, plus sophistiqué que la transcription littérale, peut être comparé au travail de terrain du sociologue ou de l'éthnographe -- en ce sens qu'elle suscite justement des questions et reconfigure les valuations de l'enquête [Dewey; Claire Lemercier, Claire Zalc] -- elle s'adosse surtout à l'élaboration de nouvelles données sur les données collectées. Ces "données sur les données" -- ces **métadonnées** --, nécessaires par exemple pour mettre en table l'information obtenue, relèveraient d'un *design* ou, autrement dit, d'une stratégie de composition de l'information en vue d'en produire une représentation intellegible [Anne-Lyse Renon]. Elle est propre à l'historien qui ajuste ces catégories à ses besoins. Si bien que ce processus valuatif de mise en catégorie de l'information capturée accompagne la transcription manuelle et littérale des sources. Elle est ce proche de la démarche éditoriale telle que décrite par Steven DeRose, David Durand, Elli Mylonas et Allen Renear, qui se représentent le texte comme une structure hiérarchique ordonnée d'objets et de contenu [Steven Derose].

Le travail de saisie manuelle, avec ou sans métadonnées, n’est évidemment pas une nouveauté introduite par l’ordinateur. Bien avant l’ère numérique, les historiens s’y adonnaient déjà. Ainsi, Pierre Chaunu qui, en 1947, recopiait à la main, sur papier, les données issues des archives microfilmées et des ouvrages nécessaires à sa thèse, afin de les ordonner et de les exploiter systématiquement [Bertrand Müller]; ou encore Laduriesous forme de fiches ou de tableaux [Ladurie]. Aujourd’hui encore, malgré l’apparition d’outils de transcription automatique [voir section 2], cette pratique demeure courante : toutes les sources ne sont pas disponibles en version numérique, et le chercheur, tout comme l’étudiant ou le généalogiste, peut être amené à relever lui-même les informations qui l’intéressent, directement en salle d’archives ou lors du dépouillement de fonds imprimés. 

Le mode opératoire de la numérisation « en mode texte » constitue en ce sens un cas exemplaire, puisqu’il s’oppose radicalement à la logique de la numérisation photographique, dite « en mode image » [Bermès]. Il ouvre également des perspectives pour le traitement quantitatif, dans la mesure où il produit une matière directement exploitable : des données susceptibles d’être structurées — par une mise en table ou un encodage hiérarchique tel que la **TEI** — et manipulées — par exemple à travers la recherche de motifs textuels.

### En "mode image"

À l’opposé du « mode texte », la numérisation en "mode image" repose sur la reproduction photographique des documents, cherchant à restituer leur matérialité visuelle : texte, blancs, marges, typographie, ornements, etc. Tout est fixé dans une matrice de pixels. Héritière des microfilms et des fac-similés, cette pratique connaît une expansion décisive dans les années 1990, avec l’essor du Web et le lancement des premières grandes campagnes institutionnelles de numérisation. Deux projets emblématiques illustrent cette dynamique : Gallica (BnF, 1997) et Google Books (2004). Leur ambition est similaire — mettre à disposition le patrimoine imprimé à grande échelle — à noter que leur logique diverge : lorsque Gallica s’inscrit dans une mission de service public, Google privilégie la puissance de l’indexation et la recherche plein texte, au détriment de l’intégrité patrimoniale des objets -- et, à ses débuts, au détriment du droit d'auteur français.

La numérisation en mode image suppose des investissements lourds en infrastructures, en personnels et en politiques documentaires. Elle se distingue ainsi des pratiques de transcription textuelle, souvent issues de gestes individuels ou collaboratifs (historien recopiant ses sources, dépouillements collectifs, corrections d’OCR par crowdsourcing). Certes, des chercheurs ou amateurs produisent eux aussi des photographies de documents — parfois propres, parfois « à l’arrache » —, mais ces fichiers restent isolés, de qualité variable, sans métadonnées ni garantie de pérennité. Ils ne deviennent pas des corpus mais des archives personnelles. Les institutions, au contraire, inscrivent leurs campagnes dans des stratégies de patrimonialisation, produisant des ensembles cohérents et diffusables à large échelle.

Sur le plan technique, ces images sont matricielles : elles fixent l’apparence de la page mais le contenu intellectuel "littéraire" demeure opaques pour l’ordinateur. Une image numérique est un tableau -- une *matrice* -- d'une largeur et d'une hauteur données, comportant alors largeur $\times$ hauteur pixels, pixels qui encode l'information colorimétrique sur trois vecteurs : le paramètre $rouge$, le paramètre $vert$, et le paramètre $bleu$. La combinaison de ces trois paramètres, selon les règles de la synthèse colorimétrique additive, permettent de restituer, pour chaque pixel, l'ensemble des couleurs du spectre visible. Qu’elles proviennent d’une campagne institutionnelle ou d’un smartphone amateur, elles ne permettent pas la recherche plein texte sans OCR ou segmentation. Dans le cas des photographies amateurs, souvent floues ou mal cadrées, l’OCR est même impraticable, réduisant ces fichiers à des fac-similés inertes, reportant ainsi la tâche de saisie manuelle non pas à partir des documents matériels, mais à partir de leur représentation numérique.

Il importe ainsi de rappeler que numériser n’est pas éditer [Bermès, Poupeau]. Là où la mise en table implique un véritable travail de curation — sélection, structuration, annotation —, la capture photographique ne livre qu’une matière brute. Sans enrichissement éditorial, sans métadonnées ni transcription automatique, ces images demeurent orphelines de leur contenu textuel : de simples objets visuels, inaccessibles à toute interrogation systématique. Leur valeur scientifique dépend donc entièrement des traitements ultérieurs qui les convertissent en données exploitables. Dans le cas de la numérisation amateure, ces fichiers constituent le plus souvent un point de départ, ou une stratégie mnémonique pour différer le travail de transcription lors de l’enquête sur les sources. À l’inverse, pour la numérisation institutionnelle, ils relèvent d’une logique patrimoniale : il s’agit avant tout de restituer des ouvrages de nature variée par la reproduction photographique, solution pragmatique mais qui pose d’emblée la question de l’alternative avec le mode texte. Comme le rappelait Jean-Didier Wagneur à propos de Gallica :

> « Nous avions le devoir patrimonial de restituer l’image du document tel qu’il a été déposé et le choix du mode image (fac-similé électronique) s’est imposé. Cette option a été à l’origine de nombreuses questions autour de l’alternative que le mode texte présentait. […] On voit qu’à terme, la saisie en mode texte aurait débouché sur la nécessité de produire des documents mixtes (texte et image) afin de préserver l’intégrité de tous les documents de nature graphique (illustrations, cartes, reproductions, graphes et expériences scientifiques) figurant dans les ouvrages numérisés. » [Jean-Didier Wagneur]

Se dessine une tension durable entre deux régimes de numérisation : d’un côté, la transcription textuelle qui repose sur la couteuse "nécessité de faire saisir les œuvres de plusieurs centaines, voire milliers, d’auteurs" exigeant un "accompagnement scientifique considérable"; de l’autre, la reproduction visuelle, portée par des politiques institutionnelles, qui fabrique des corpus massifs mais souvent réduits à l’état de matière brute.

### La "datafication"

Cette opposition doit enfin être replacée dans l’horizon plus large de la *datafication* (Clavert, Rygiel). La *datafication* est le processus qui vise à quantifier un phénomène de sorte qu'il soit calculable et analysable [Frédéric Clavert]. Elle est en quelque sorte un "schème opératoire" [Simondon] permettant la calculabilité des sources avec un outillage informatique. Cette mise en données "insiste sur la notion de processus" et "se définit par les choix opérés par les organismes qui y procèdent", cela impliquant "les critères d'inclusion [de] corpus à numériser"; l'élaboration de métadonnées descriptives situées, lesquelles ont un impact sur leur découvrabilité puisque les moteurs de recherche s'y appuient [Frédéric Clavert, 123].

Si bien que numériser, ce n’est pas seulement reproduire. C’est transformer des artefacts en données, dans un cadre technique, social et institutionnel qui oriente les usages, fixe les normes de conservation et conditionne l’accès même aux sources. Elle met en évidence deux conceptions distinctes de la numérisation: d’un côté, la transcription textuelle, qui construit les données par un travail de sélection et de structuration; de l’autre, la reproduction visuelle, qui se limite à conserver une représentation plane de la matérialité de l’objet. L’histoire des pratiques documentaires de "mise en données" témoigne de cette tension durable entre deux paradigmes concurrents [Bermès, 29] qui sont alors autant techniques qu'institutionnels. En suivant Bruno Latour, on peut dire que "nous ne devrions jamais parler de ‘données’, mais toujours d'‘obtenues’" [Datactivist]. La saisie manuelle n'est pas une simple transplantation de contenu, mais une construction — un fait mobilisable conditionné par des choix de catégorisation, de format, et d’usage. La reproduction photographique, à l’inverse, ne produit pas directement de données exploitables, mais fige l’apparence visuelle du document, laissant le texte dans un état opaque tant qu’aucune opération d’extraction ou d’annotation n’est réalisée. 

Chaque "mise en données" est ainsi moins une instanciation pure de la source dans le giron du binaire qu'une stratégie épistémique de sa présentation au sein d’un réseau sociotechnique instituant [Anne-Lyse Renon]. Dans cette perspective, on peut dire avec Cornelius Castoriadis que la numérisation est un geste "instituant" : elle crée de nouvelles manières de faire exister et de rendre visibles les sources, conditionnant les régimes d’intelligibilité qui en découlent. Or, au sein des politiques contemporaines de numérisation, cette dimension instituante prend une forme discursive et normative particulière : celle du *patrimonial*. La numérisation est justifiée et orientée par un vocabulaire de conservation, de démocratisation et de valorisation. Autrement dit, ce qui est numérisé n’est pas seulement conservé ou reproduit : il est institué comme patrimoine, doté d’une valeur symbolique "émotionnelle" et sociale spécifique [Bermès]. Les "données" produites par la numérisation sont donc indissociables d’une politique culturelle qui configure les conditions d’accès, de visibilité et de réutilisation des corpus, ce qui implique nécessairement des biais de sélection ou tout simplement des priorités déterminées par les politiques de conservation de documents fragiles.

### Pratiques historiennes : sphère technique, sphère sociale

La datafication ne consiste donc pas seulement en un enchaînement d’opérations techniques — OCR, structuration, encodage —, mais en un processus situé, au croisement de la sphère technique et de la sphère sociale. Il n’existe pas de "données brutes" : toute donnée est déjà une *capta*, c’est-à-dire une information "prise", construite dans et par un cadre interprétatif qui reflète des choix méthodologiques et institutionnels [Johanna Drucker]. Leur mise à disposition est elle-même également une affaire de visualisation : la mise en ordre des résultats d'une recherche en ligne appartient au giron de la visualisation des données -- et comme on l'a sous entendu précédemment, les stratégies épistémiques de représentation de connaissance est une affaire de *design*. D'ailleurs certains artistes, parlant volontiers de "nouvel espace épistémique" ou "d'*épistémè numérique*", ont justement mis au centre de leur travail cette question "interprétative" et graphique de l'accès à l'information à travers des interfaces [Architecture de Mémoire, Jean-Marie Dallet].

Ces interrogations contemporaines ne surgissent pas *ex-nihilo*: elles prolongent une histoire plus ancienne des rapports entre historiens et outils informatiques. Ces transformations récentes s’inscrivent dans une histoire plus longue des usages de l’informatique par les historiens. Dès les années 1950, des expériences pionnières lient mécanographie et analyse sérielle [Adeline Daumard, François Furet], dans la lignée des « archives quantitatives » chères à l’école des Annales. Les décennies 1960–1970 voient s’imposer un véritable culte du chiffre, nourri par l’accès aux centres de calcul et par l’ambition d’une histoire totale [Sébastien Poublanc, Nicolas Marqué]. La prophétie de Le Roy Ladurie — « l’historien de demain sera programmeur ou ne sera pas » — illustre cet horizon, même si les limites de l’histoire sérielle conduisent rapidement à relativiser l’objectivité promise par la machine. Dans les années 1980–1990, l’apparition du micro-ordinateur personnel, puis des premières bases de données relationnelles et de revues comme *Le Médiéviste et l’Ordinateur*, favorise une technicisation diffuse des pratiques, souvent portée par des chercheurs passionnés plutôt que par une politique disciplinaire globale [Sébastien Poublanc, Nicolas Marqué]. Le Web, à partir de la fin des années 1990, change l’échelle de ces usages : il facilite l’accès et la diffusion des corpus (Gallica, revues.org) et transforme le rapport aux archives grâce aux appareils photo numériques et aux interfaces de recherche. L’émergence des humanités numériques dans les années 2000 fait de ces pratiques dispersées un champ identifié, mais aussi un espace de tensions : entre injonctions institutionnelles, résistances disciplinaires et négociations interdisciplinaires. L’histoire des pratiques historiennes « numériques » apparaît ainsi comme un processus long, fait d’allers-retours entre engouements, critiques et réinventions, qui relativise l’idée d’un tournant soudain pour insister sur la continuité d’une adaptation progressive des historiens à leurs outils.

Avec l’essor du Web, la datafication ne se réduit plus à un geste de transcription ou à un choix institutionnel de numérisation : elle s’inscrit dans des infrastructures réticulaires [Bernard Stiegler] qui conditionnent la circulation et l’usage des corpus. Les documents ne sont pas seulement mis en ligne ; ils sont exposés à travers des protocoles techniques — comme les API — qui déterminent la granularité d’accès, la possibilité de réutilisation et la manière dont les sources sont découvertes. Comme le montre l’exemple du *Goût de l’archive à l’ère numérique* et de sa réflexion sur le "goût de l’API", l’archive numérisée devient un objet relationnel : elle n’existe pleinement qu’à travers les réseaux qui l’indexent, la connectent et la rendent interopérable avec d’autres ensembles de données. Cette dimension réticulaire transforme profondément la sphère sociale des archives : les corpus ne sont plus simplement conservés et transmis, mais distribués, exposés, parfois fragmentés, selon des logiques de plateformes et de moteurs de recherche. La découvrabilité des sources dépend ainsi de ces dispositifs techniques saisis dans des processus de concrétisation [Simondon], qui agissent comme de nouveaux médiateurs documentaires et configurent, en amont, les conditions de possibilité de l’enquête historienne. 

Autrement dit, ce que fait la numérisation aux corpus, c’est moins de les rendre "disponibles" que de les reconfigurer: par la sélection de ce qui est numérisé (et de ce qui ne l’est pas), par les formats qui conditionnent l’usage (XML-TEI, bases relationnelles, IIIF), et par les réseaux de diffusion (catalogues, moteurs de recherche, portails institutionnels) qui hiérarchisent leur visibilité. La donnée numérique n’est donc pas un miroir fidèle des sources, mais une construction sociotechnique qui oriente leur appropriation. De ce point de vue, la datafication prolonge les silences de l’archive autant qu’elle ouvre de nouvelles potentialités. Elle produit un double effet : d’un côté, elle consolide des corpus institués par les politiques patrimoniales et documentaires ; de l’autre, elle institue de nouveaux régimes de visibilité et de calcul, rendant possible des analyses sérielles, des croisements de données ou des visualisations inédites. C’est dans cette tension que se joue aujourd’hui la confiance des chercheurs dans les "données" numériques : non comme transparence des sources, mais comme résultat de choix techniques et sociaux qu’il convient de rendre visibles et discutables. La datafication amplifie ainsi certains silences archivistiques : ce qui n’a pas été consigné, ou ce qui est difficile à transcrire automatiquement, reste hors champ.

Enfin, la datafication est aussi un processus social : elle reflète et prolonge les hiérarchies documentaires héritées. Les corpus numérisés surreprésentent souvent les groupes dominants (élites, institutions, employeurs), au détriment des voix minoritaires. Le danger est alors de tomber dans une réification de la disponibilité où l’historien travaille sur ce qui est disponible, non sur ce qui est historiquement pertinent ou accessible. La question devient dès lors : comment documenter ces biais et construire la confiance dans des données issues de systèmes techniques ? 

### Numérisation du *Journal Officiel* : entre politique documentaire, infrastructures techniques et souveraineté archivistique

Du côté de la BnF, avec le projet Gallica, c'est le "mode image" qui a été choisi : "la bilbliothèque se range pour de bon du côté de la reproduction plutôt que de l'édition".

La numérisation du *Journal Officiel de la République française* (J.O.) constitue un cas exemplaire des tensions de la datafication. Sa mise en ligne sur **Gallica**, la bibliothèque numérique de la BnF, n’est pas seulement le produit d’une opération technique (scanner, OCR, structurer) : elle relève d’une **politique documentaire explicite**, d’une hiérarchie patrimoniale et d’une réflexion sur la transparence démocratique.

Dès le lancement de Gallica dans les années 1990, les corpus officiels et juridiques ont été considérés comme prioritaires dans les programmes de numérisation. La BnF a défini une **politique de numérisation concertée** qui associe ses partenaires institutionnels (bibliothèques, archives, musées, mais aussi administrations parlementaires) et repose sur deux modalités principales :

* la **subvention**, qui permet à des institutions tierces de financer la numérisation de leurs fonds à condition que les résultats soient interopérables et intégrés dans Gallica ;
* l’**intégration directe** dans le marché de numérisation de la BnF, réservée aux corpus volumineux ou emblématiques, comme le *Journal Officiel*, qui nécessitent une infrastructure robuste et centralisée ([BnF, *Numérisation concertée de corpus imprimés*, 2018](https://www.bnf.fr/sites/default/files/2018-11/num_concertee_impr_progr_partenaires.pdf?utm_source=chatgpt.com)).

Le choix du J.O. s’explique à la fois par sa **valeur patrimoniale** (trace officielle de la vie normative de l’État depuis 1870), son **utilité sociale et démocratique** (garantir un accès transparent au droit), et par son **homogénéité formelle** qui facilite les opérations techniques (OCR, segmentation par rubriques, enrichissement par métadonnées). La mise en ligne du J.O. sur Gallica couvre aujourd’hui de larges pans de la Troisième et de la Quatrième République, même si certaines années restent lacunaires ou peu visibles ([Boîte à Outils, 2013](https://boiteaoutils.info/2013/01/acceder-aux-numerisations-du-journal/?utm_source=chatgpt.com)).

Cette politique s’accompagne d’une **chaîne technique complexe** :

* numérisation en mode image (PDF, JPEG) ;
* reconnaissance optique de caractères (OCR), dont la qualité varie selon la typographie, l’état du papier ou la mise en page ;
* segmentation en unités documentaires (articles, décrets, rubriques) ;
* enrichissement par métadonnées, qui conditionne la découvrabilité dans les moteurs de recherche.

Ces étapes introduisent des biais : erreurs OCR qui faussent la recherche plein texte, segmentation parfois incomplète, normalisation qui gomme des variations de présentation. Le J.O. numérisé n’est donc pas un « miroir » de la source papier, mais un **objet reconfiguré** par la chaîne sociotechnique de la BnF.

### Sénat et Assemblée nationale : des acteurs de la numérisation parlementaire

Le processus ne relève pas uniquement de la BnF. Les **deux chambres du Parlement** sont parties prenantes de cette politique de numérisation, en lien étroit avec Gallica.

* Le **Sénat** a engagé la numérisation de ses **Impressions parlementaires** (débats, annexes, rapports) couvrant la Troisième République. Plusieurs campagnes ont permis d’intégrer dans Gallica des volumes allant de 1876 à 1905, puis de 1910 à 1940, avec un travail en cours sur 1906–1909 ([Sénat.fr](https://www.senat.fr/connaitre-le-senat/lhistoire-du-senat/les-travaux-du-senat-de-la-troisieme-republique.html?utm_source=chatgpt.com)).
* L’**Assemblée nationale** a, de son côté, collaboré à la mise en ligne de ses débats parlementaires. Sur **Retronews**, on trouve les comptes rendus de séances entre 1881 et 1939, tandis que le site de l’Assemblée propose aujourd’hui l’accès aux débats de la IVᵉ République jusqu’à l’époque contemporaine, avec une recherche plein texte ([BnF LibGuides](https://bnf.libguides.com/c.php?g=659907&p=4659962&utm_source=chatgpt.com); [Boîte à Outils](https://boiteaoutils.info/2013/01/acceder-aux-numerisations-du-journal/?utm_source=chatgpt.com)).

Ces initiatives montrent que le Parlement n’est pas un simple producteur de données, mais aussi un **acteur documentaire** qui oriente la sélection, la structuration et la diffusion de ses propres archives. Le croisement entre Gallica, Retronews et les sites institutionnels illustre l’existence d’**écosystèmes documentaires pluriels**, qui médiatisent différemment un même corpus selon les publics visés (chercheurs, citoyens, journalistes).

### Archives numérisées et archives nativement numériques : le cas du JORF

Il convient enfin de distinguer deux régimes d’archives :

* Les **archives numérisées**, comme les volumes historiques du J.O. : elles proviennent d’un support papier, transformé par une chaîne technique (numérisation, OCR, indexation). Leur fiabilité est conditionnée par la qualité des scans et des traitements automatiques, et leur diffusion par les choix de formats (PDF image, texte OCRisé, métadonnées).
* Les **archives nativement numériques**, comme le *Journal officiel de la République française* contemporain (JORF), désormais produit directement sous forme numérique, structuré en XML, interrogeable via des bases de données et accessible via **Légifrance**. Ici, il n’y a pas de passage par l’OCR : les textes sont disponibles en clair, immédiatement exploitables, interopérables et consultables en temps réel.

Cette distinction a des conséquences méthodologiques majeures :

1. **Qualité et fiabilité** : les données du JORF sont plus stables, car issues d’une chaîne de production numérique native.
2. **Temporalité d’accès** : le JORF offre une mise à disposition quasi instantanée, là où les corpus rétro-numérisés accusent des délais et des lacunes.
3. **Structuration** : l’usage du XML et des API ouvre de nouvelles possibilités d’exploitation automatique, d’agrégation et de visualisation.

Ainsi, la numérisation du J.O. historique et la production numérique du JORF contemporain dessinent deux faces d’une même logique : d’un côté, la reconfiguration patrimoniale d’archives imprimées par la BnF ; de l’autre, la fabrique d’archives nativement numériques par l’État. Dans les deux cas, ce sont des **infrastructures sociotechniques** qui conditionnent la circulation, la visibilité et la confiance dans les données.

# En passant par le texte : de l'image à la donnée structurée

Doubler les images du texte.

Le choix institutionnel de numériser massivement les corpus patrimoniaux en mode image tient à la volonté de restituer au plus près la matérialité visuelle des documents. Dès les années 1990, les grandes bibliothèques nationales — à commencer par la BnF avec Gallica — ont privilégié la photographie ou le scan intégral des pages afin de garantir une reproduction fidèle : typographie, mise en page, ornements, blancs, marques de lecture ou d’usage. Cette approche s’inscrit dans une logique de patrimonialisation : conserver une trace stable et exploitable de l’objet imprimé, indépendamment des évolutions des standards textuels ou des logiciels de lecture. L’image constitue en effet un témoin pérenne, qui permet aussi bien la consultation visuelle du document par le lecteur que la réédition de fac-similés numériques. Mais cette stratégie, qui assure l’intégrité visuelle et symbolique de la source, laisse le texte dans un état opaque pour la machine. C’est précisément pour franchir cette opacité et rendre ces corpus interrogeables en plein texte qu’intervient l’OCR (Optical Character Recognition), technologie pivot entre la reproduction visuelle et la transformation en données exploitables. Dans ce chapitre, nous reviendrons sur les aspects techniques de l'OCR.

### OCR Extraire automatiquement du texte

La reconnaissance optique de caractères (OCR, *Optical Character Recognition*) désigne l’ensemble des procédés permettant d’extraire automatiquement du texte lisible par machine à partir d’images numérisées. Elle constitue l’opération clef qui rend un document photographié ou scanné interrogeable en plein texte. Concrètement, l’OCR vise à transformer une matrice de pixels (mode image) en une suite de signes discrets (mode texte), selon la logique de la *datafication* déjà évoquée : passage d’un continu visuel à un discret alphabétique.

Un document numérisé est d’abord une matrice de pixels codant des intensités lumineuses. Pour l’ordinateur, un caractère imprimé n’existe pas en tant que lettre, mais comme une forme graphique composée de zones plus ou moins sombres. L’OCR consiste à :

1. **Prétraiter l’image** (binarisation, redressement, suppression du bruit visuel, segmentation en zones de texte, lignes et mots) afin d’obtenir des silhouettes de caractères.

2. **Reconnaître les caractères** par comparaison avec des modèles préexistants. Deux approches coexistent :
   
   * la reconnaissance par *patterns* (méthodes classiques, basées sur la correspondance visuelle de formes) ;
   * la reconnaissance statistique et neuronale (méthodes modernes, utilisant apprentissage profond et réseaux de neurones convolutifs pour apprendre les formes typographiques).

3. **Restituer un texte** sous forme encodée (UTF-8 ou Unicode), éventuellement enrichi de métadonnées de position (*ALTO XML*, *hOCR*) permettant de conserver l’ancrage spatial des mots dans l’image.

Le résultat dépend de nombreux facteurs : qualité du scan, état matériel du document, typographie, langue, mais aussi du degré d’adaptation du modèle de reconnaissance aux sources traitées.

### Les grandes technologies d’OCR

Historiquement, les premiers systèmes d’OCR (années 1970–1990) étaient conçus pour reconnaître des caractères bien standardisés (typographies contemporaines, impressions propres). Avec l’essor de la numérisation patrimoniale et la variété des fonds (imprimés anciens, manuscrits, journaux abîmés), les limites de ces approches ont conduit au développement de solutions spécialisées.

* **Tesseract OCR** : développé par HP dans les années 1980 puis repris par Google, Tesseract est l’un des moteurs les plus utilisés, notamment dans les grandes bibliothèques numériques (Google Books, Internet Archive). Depuis sa version 4 (2018), il intègre des réseaux de neurones récurrents (LSTM), améliorant sa performance sur des typographies variées. Cependant, il reste peu flexible sur des écritures complexes ou des documents abîmés.

* **ABBYY FineReader** : solution propriétaire largement utilisée par les institutions pour sa robustesse industrielle. Performante sur les imprimés modernes, elle demeure coûteuse et peu ouverte, limitant sa personnalisation.

* **Kraken** : développé à partir de l’expérience d’*OCRopus* (projet open-source de Google), Kraken est un moteur open-source basé sur l’apprentissage profond. Sa force réside dans la possibilité d’entraîner des modèles sur des corpus spécifiques (par exemple, typographies anciennes, textes en alphabets non latins). Il est aujourd’hui largement utilisé pour les fonds patrimoniaux et manuscrits.

* **eScriptorium / Pero OCR** : environnement développé à l’École pratique des hautes études (EPHE/PSL) et au sein du projet *Huma-Num*. Il combine des outils de segmentation (basés sur les réseaux de neurones convolutifs de Pero OCR) et de transcription (via Kraken). eScriptorium fournit une interface collaborative pour annoter, entraîner des modèles et produire des transcriptions à grande échelle. Cette approche est particulièrement adaptée aux humanités numériques, car elle permet d’impliquer chercheurs et communautés dans l’amélioration des modèles.

* **HTR (Handwritten Text Recognition)** : pour les manuscrits, l’OCR cède la place à la HTR, qui repose sur des principes analogues mais adaptés aux écritures manuscrites. La plateforme **Transkribus**, par exemple, est aujourd’hui la référence pour les fonds manuscrits européens, permettant d’entraîner des modèles spécifiques sur des écritures d’archives.

### Enjeux et limites

Les performances de l’OCR sont hétérogènes : sur des imprimés du XXᵉ siècle en bon état, la reconnaissance atteint souvent plus de 95 % d’exactitude. En revanche, pour des imprimés anciens (XVIᵉ–XVIIIᵉ siècles), des typographies gothiques ou des journaux dégradés, le taux d’erreur peut devenir très élevé. Cela implique :

* des corrections manuelles ou collaboratives (*crowdsourcing*),
* l’entraînement de modèles spécialisés,
* l’élaboration de pipelines de traitement intégrant segmentation, normalisation linguistique et encodage structuré (TEI, ALTO XML).

L’OCR ne produit donc jamais une "donnée brute", mais un texte *obtenu*, dont la fiabilité dépend du corpus, des choix techniques et du travail éditorial qui l’accompagne.

Comme on l'a vu dans le chapitre précédent, les *Tables Annuelles* du Sénat sont disponibles sur Gallica. D'un point de vue technique, ces *Tables* sont des documents numérisés, c'est-à-dire des images dont on aura discrétisé l'information. 

# Données brutes, données structurées : quelques enjeux de l'interopérabilité.

En prolongement de notre problématique initiale, nous structurons notre revue autour de trois questions clés :

1. Comment modéliser efficacement des données structurées ?

2. Comment évaluer la qualité des données structurées produites par ces approches ?

3. Comment générer des données structurées à partir de texte ?

### Modéliser des données structurées

Les données structurées peuvent prendre plusieurs formes, parmi lesquelles :

- **Ensembles d’enregistrements (Record Sets)** : ce sont des ensembles non ordonnés de tuples, analogues à des tables de base de données où les colonnes représentent les attributs des objets. Cette structure est couramment utilisée dans les tâches d’extraction d’information, comme la reconnaissance d’entités nommées [18] ou l’extraction de relations [15].

- **Séquences d’enregistrements (Record Sequences)** : ce sont des versions ordonnées des ensembles d’enregistrements, où l’ordre des éléments a un sens. La séquence peut refléter un critère (par exemple temporel) ou faciliter certaines tâches comme la validation croisée, à l’image des annuaires.

- **Arbres** : ces structures hiérarchiques sont souvent utilisées pour représenter des relations imbriquées ou des dépendances, comme en analyse syntaxique de dépendances [14].

- **Graphes** : structures flexibles utilisées pour représenter des connaissances consolidées, comme les ontologies ou les graphes de connaissances. Bien qu’ils soient largement étudiés, leur évaluation sort en général du champ de l’extraction d’information, et dépasse donc le cadre de ce travail.

Dans cet article, nous nous concentrons sur les ensembles et séquences d’enregistrements, qui sont les plus pertinents pour notre étude de cas : l’extraction de données structurées à partir de documents parlementaires. Les différents modèles que nous avons expérimentés sont décrits en section 3.

### Évaluer la qualité des données structurées

L’évaluation de la qualité des données structurées peut être globalement classée en deux familles de métriques : les métriques de type distance d’édition et les métriques d’appariement, telles que définies dans [2].

- **Métriques de distance d’édition** : elles reposent sur des processus d’optimisation complexes et sont souvent coûteuses en calcul. Leur interprétabilité est limitée, car elles ne fournissent pas de comparaison directe entre données produites et attendues. Exemples : la distance de Levenshtein (pour les comparaisons caractère par caractère), ou la distance d’édition d’arbres [25] pour des structures hiérarchiques. Des métriques générales existent aussi pour les graphes, mais leur complexité les rend souvent impraticables.

- **Métriques d’appariement** : plus interprétables, elles identifient explicitement les éléments qui correspondent entre les données produites et attendues. La plupart reposent sur un appariement biparti entre les ensembles prédits et de référence, et calculent des scores à partir du nombre d’éléments appariés [2]. Les plus courantes incluent la mesure F1 (précision + rappel) et l’indice de Jaccard (similarité d’ensembles). Peu d’études cependant se concentrent sur les cas de données structurées ou d’appariement partiel, où les données produites ne coïncident pas parfaitement avec les attendues.

Il est intéressant de noter que la communauté de la vision par ordinateur rencontre exactement le même problème. Par exemple, le *COCO Panoptic Segmentation Challenge* [7] propose un cadre similaire, où l’évaluation combine détection, segmentation et classification via un appariement optimal entre les surfaces prédites et de référence. Une démarche analogue est adoptée par Chen et al. [2].

Dans nos travaux, nous adoptons une métrique d’appariement basée sur un appariement optimal entre ensembles de données structurées. Cette approche généralise l’appariement biparti tout en intégrant les correspondances partielles. Elle fournit à la fois une évaluation quantitative de la qualité des données et une identification des éléments manquants ou « hallucinés », offrant ainsi des pistes d’amélioration concrètes.

### Produire des données structurées à partir de texte

Les approches de génération de données structurées à partir de texte se divisent en deux grandes catégories : **détection (extractive)** et **génération (abstractive ou générative)**.

- **Approches extractives** : elles identifient dans le texte les fragments correspondant à des champs ou éléments de la donnée structurée. Les méthodes traditionnelles reposaient sur des règles (expressions régulières, heuristiques). Plus récemment, des modèles d’apprentissage, comme les modèles de séquence étiquetée (par ex. CRF [5]) ou les modèles encodeurs-transformers (par ex. BERT [4]), dominent. Leur conception impose un alignement strict entre texte d’entrée et étiquettes de sortie, réduisant le risque d’hallucinations (c’est-à-dire d’inventions de données). Mais ces approches nécessitent un entraînement spécifique à la tâche, donc des données annotées, des ressources computationnelles et du temps.

- **Approches génératives** : elles exploitent des modèles autoregressifs pour « traduire » le texte en un format structuré cible. L’essor des LLMs a ravivé l’intérêt pour cette voie, en raison de leurs capacités de généralisation impressionnantes, même sans entraînement spécifique [1, 20, 21]. Les sorties peuvent être contraintes à des formats précis (JSON, schémas complexes) grâce au filtrage dynamique de tokens valides [23]. Ces modèles peuvent produire des structures complexes et imbriquées, et inférer des éléments implicites. Leur inconvénient majeur est cependant leur propension aux hallucinations, difficiles à détecter.

Dans cet article, nous explorons l’efficacité des approches génératives pour gérer les structures répétitives présentes dans les index parlementaires ou *Tables*. Nous mettons à profit les capacités *zero-shot* des LLMs et évaluons la viabilité de cette approche pour générer des données structurées dans ce contexte spécifique.

# Du texte à la donnée structurée : capturer la sémantique

## Approche à motifs explicites : les ReGex

Une première approche naïve d'extraction de l'information du texte : les regex. Puissants, rapides. Mais rigide et implique de connaître à l'avance la forme de ce qu'on cherche, ce qui n'est pas trivial ! Il faut aussi partir du principe que l'on a pas une connaissance synthétique a priori de l'information. Il y a toujours un "hic". Fragile face au bruit ocr, aux fautes typographiques inattendues; et avoir une regex plus souple, c'est aussi prendre le risque de capter du bruit.

> La recherche floue Un moyen de diluer la rigidité des motifs; mais ne permet que de trouver ce que l'on connaît à l'avance. Dans un optique d'extraction massive, on veut tout sortir automatiquement.
> 
> > Automates finis !

La contrainte forte des regex Intéressant à coupler avec d'autres approches plus souple comme on le verra.

- **Patrons linguistiques** (grammaires, dépendances syntaxiques)

- **Listes de référence / gazetteers**

- **Règles de post-traitement**  
  => Avantage : explicable, prévisible  
  => Limite : peu robustes aux variations inattendues

## Approches extractives : l'approche Bert (one-to-one)

L'approche du surlignage 1 to 1.

**Principe** : le modèle apprend à repérer les entités dans un texte via des annotations.

- **Modèles supervisés classiques** : CRF, SVM, MaxEnt

- **Neuraux séquentiels** : BiLSTM-CRF, CNN-LSTM

- **Transformers extractifs** : BERT, RoBERTa, CamemBERT en mode NER  
  => Avantage : généralise mieux, bonne précision  
  => Limite : nécessite des données annotées et un entraînement

## Approches génératives : les LLMs

L’usage croissant de l’intelligence artificielle par les historiens \[3] multiplie les possibilités de production de jeux de données historiques. L’avènement des grands modèles de langage (LLMs) modifie encore davantage le paysage, en particulier pour le traitement des corpus textuels, avec une prolifération d’usages et d’expérimentations dans les sciences humaines et sociales¹. Les LLMs en *zero-shot* sont capables d’accomplir une large gamme de tâches sans nécessiter d’exemples spécifiques à la tâche ni de réglages fins \[12, 22, 26], et ont démontré leur capacité à effectuer de nombreuses opérations chronophages de la recherche historique, telles que la transcription \[6], l’extraction d’information \[9], ou encore l’annotation \[24].

L’utilisation des LLMs ouvre de nouvelles perspectives pour l’extraction de données structurées \[13] à partir de documents historiques. Dans ce contexte, un défi central réside dans la production de sorties structurées. Une première approche consiste à contraindre la génération d’un modèle de langage pour qu’il produise directement l’information dans un format prédéfini (par exemple, du JSON). À l’inverse, la structure peut être imposée par un post-traitement de sorties textuelles libres. Quelle que soit l’approche, la production de données structurées permet d’assurer la traçabilité jusqu’au document original et de faciliter la vérification des sources. Elle soutient également des usages en aval, tels que l’intégration dans une base de données ou l’analyse computationnelle.

Deux questions fondamentales demeurent cependant :

1. comment passer d’un texte brut à une représentation structurée exploitable, telle qu’un tableau ou un fichier CSV ;
2. comment évaluer la qualité et la fiabilité des données extraites.

Cet article aborde ces deux aspects à travers une étude de cas concrète : l’extraction d’informations structurées à partir des *Tables nominatives* (ou *Tables des noms*) du Sénat français de 1931, qui constituent un index de l’activité parlementaire classé par nom. Nous explorons une approche de génération faiblement contrainte à l’aide d’un LLM, et proposons une méthode pour représenter les données cibles, guider le processus d’extraction et évaluer les performances du système. Au-delà de ce cas spécifique, l’étude vise à contribuer à une réflexion plus large sur la faisabilité et les limites des modèles génératifs pour la structuration de données historiques.

Les *Tables des noms* du Sénat français furent publiées durant la Troisième République (1870–1940)². Dans l’écosystème documentaire plus large du *Journal Officiel* — qui vise à reconstituer l’activité parlementaire et ses issues juridiques ou réglementaires en France —, les *Tables nominatives* du Sénat offrent un relevé concis et systématique des interventions des sénateurs en séance publique. Ces index étaient conçus pour accompagner la transcription des débats³ et en faciliter la consultation. Compilés manuellement une fois par an, ils recensent chaque intervention d’un sénateur ou d’un membre du gouvernement, précisent l’objet de son discours et indiquent la pagination correspondante. Si ces tables étaient particulièrement utiles à une époque où la recherche plein texte dans les débats parlementaires numérisés n’était pas possible, elles conservent aujourd’hui encore une forte valeur pour les historiens. Une extraction systématique des données permettrait de suivre l’activité parlementaire sur le long terme, de quantifier les interventions de sénateurs affiliés à certains mouvements politiques, ou encore de soutenir la validation croisée des entités nommées extraites des débats eux-mêmes.

Notre objectif est d’extraire des données structurées de ces tables ; pour nos premières expérimentations, nous nous concentrons sur une seule table nominative, celle de 1931. Le début des années 1930 marque en effet l’entrée du parlementarisme français dans une phase de déclin, qui culmine avec la chute de la Troisième République en 1940 \[17]. L’analyse de la table de 1931 permet de poser les bases d’une étude élargie à l’ensemble de la décennie, afin de mieux saisir l’activité parlementaire du Sénat puis, ultérieurement, celle de la Chambre des députés.

Après un état de l’art des approches existantes pour l’extraction et l’évaluation de données structurées (## Travaux connexes), nous présentons trois contributions principales :

1. nous concevons et mettons en œuvre une chaîne de traitement guidée par un schéma, pour extraire des informations structurées à partir des *Tables nominatives* du Sénat, en combinant OCR et génération à base de prompt avec un LLM (## Schéma et pipeline) ;
2. nous introduisons un protocole d’évaluation adapté à cette tâche, incluant une méthode d’alignement par appariement optimal et une métrique continue qui prend en compte les sorties partielles et bruitées (## Protocole d’évaluation) ;
3. nous fournissons une évaluation empirique de l’extraction par LLM dans ce contexte historique, montrant que les performances du modèle sont fortement influencées par la conception du prompt et du schéma de données — deux éléments que nous proposons de considérer comme des paramètres critiques du processus de modélisation global (## Résultats).

**Principe** : le modèle produit directement le résultat structuré à partir du texte, sur la base d’une consigne en langage naturel.

- **LLMs** (GPT, Claude, Mistral) en extraction via prompt

- **Fine-tuning génératif** (T5, GPT-4 en mode extraction JSON)  
  => Avantage : très flexible, pas besoin de jeu d’entraînement spécialisé  
  => Limite : variabilité, hallucinations, besoin de validation

## Approches hybrides

**Principe** : combiner plusieurs catégories dans un flux de traitement.

- Exemple : Gazetteer pour repérer des entités connues + BERT pour les autres + Regex pour les formats normés + validation humaine  
  => Avantage : maximiser précision et rappel  
  => Limite : complexité d’intégration

## La sortie structurée via LLM pour la capture sémantique du Journal Officiel

Expliquer le choix des LLMs (rapide et pas cher + pas d'entrainement ou de spécialisation).
