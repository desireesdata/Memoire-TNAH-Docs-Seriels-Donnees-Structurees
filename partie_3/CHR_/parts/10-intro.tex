\section{Introduction}
\label{sec:intro}
The growing use of artificial intelligence by historians \cite{clavert2024histoire} is multiplying the possibilities for producing historical datasets. The advent of large language models (LLMs) is further changing the landscape, especially for processing textual data corpora, with a proliferation of uses and experiments in the humanities and social sciences\footnote{The "DH@LLM: Grands modèles de langage et humanités numériques" conference program, held in Paris in July 2025, is a good illustration of this: \href{https://www.crihn.org/nouvelles/2025/01/16/colloque-dhllm-grands-modeles-de-langage-et-humanites-numeriques-sorbonne-universite/}{https://www.crihn.org/nouvelles/2025/01/16/colloque-dhllm-grands-modeles-de-langage-et-humanites-numeriques-sorbonne-universite/}}. Zero-shot LLMs are capable of performing a wide range of tasks without the need for task-specific examples or fine-tuning \cite{kojima2022large,wei2022emergent,zhao2023survey} and have demonstrated their ability to carry out many time-consuming tasks in historical research, such as transcription \cite{humphries2024unlocking}, information extraction \cite{knutsen2024alimenter}, or annotation \cite{yuan2025leveraging}. 

%In the context of historical data extraction, a key issue is obtaining structured outputs. While structured generation -- which constrains a Large Language Model to directly produce information in a predefined format like JSON -- is a way to achieve this, a structured output can also be obtained through post-processing of free-form text generation. Regardless of the method, structured output enables the preservation of \joseph{the link to?} the original document and facilitates return to the source, while still allowing for subsequent processing — such as, for instance, integration into a database.

The use of large language models (LLMs) opens new perspectives for extracting structured data \cite{liu2024structured} from historical documents. In the context of historical data extraction, a central challenge lies in obtaining structured outputs. One approach is structured generation, which constrains a large language model to directly produce information in a predefined format such as JSON. Alternatively, structure can be imposed through post-processing of free-form text outputs. Regardless of the approach, producing structured data enables traceability back to the original document and facilitates source verification. It also supports downstream uses, such as integration into a database or further computational analysis.

%\joseph{Est-ce que le problème n'est pas de dire : 1. comment on passe du texte brute au CSV (pour simplifier) et 2. comment on évalue ce que ça donne ?}

%\marie{Si ça vous convient à tous les deux, je trouve ça mieux.}
%\joel{Oui, ça me convient !} \marie{Ci-dessous, une proposition, en ayant utilisé ce qu'a dit Joseph. J'ai aussi utilisé ce qu'il a dit par Teams.}

Two fundamental issues still remain: (1) how to move from raw text to an exploitable structured representation, such as a table or CSV file ; and (2) how to assess the quality and reliability of the extracted data. This article addresses both aspects through a concrete case study: the extraction of structured information from the 1931 \textit{Tables nominatives} or \textit{Tables des noms} of the French Senate (index of senatorial activity ordered by name). We explore a lightly constrained generation approach using an LLM and propose a method to represent the target data, guide the extraction process, and evaluate system performance. Beyond this specific case, the study aims to contribute to broader reflections on the feasibility and limitations of generative models for structuring historical data.

The \textit{Tables des noms} of the French Senate was published during the French Third Republic (1870–1940)\footnote{These tables are part of the \textit{Tables annuelles} (yearly activity index), which can be consulted on the digital library of the French national library (\textit{BnF}): \href{https://gallica.bnf.fr/ark:/12148/cb371291967/date.item}{https://gallica.bnf.fr/ark:/12148/cb371291967/date.item}.}.
Within the broader documentary ecosystem of the \textit{Journal Officiel} ---~which seeks to reconstruct parliamentary activity and its legal or regulatory outcomes in France~---, the Senate's \textit{Tables nominatives} offer a concise and systematic record of senators' interventions during public sessions.
These indexes were designed to accompany the transcription of debates\footnote{The complete transcriptions of Senate debates can be consulted via Gallica: \href{https://gallica.bnf.fr/ark:/12148/cb34363182v/date}{https://gallica.bnf.fr/ark:/12148/cb34363182v/date}.} and to facilitate their consultation.
Manually compiled once a year, they recorded each intervention by senators or members of the government who spoke during the sessions, the subject of their speech, and the corresponding page number.
%
While these tables were particularly useful at a time when full-text search in digitized parliamentary debates was not possible, they still hold significant value for historians today.
Systematically extracting data from them would make it possible to track parliamentary activity over the long term, quantify the interventions of specific senators affiliated with particular political movements, or support the cross-validation of named entities extracted from the debates themselves.
%
Our objective is to extract structured data from these \textit{Tables}; for our initial experiments, we focus on a single \textit{Table nominative}, namely that of 1931.
The early 1930s marked the beginning of the decline of French parliamentarism, culminating in the fall of the Third Republic in 1940 \cite{morel2024parlement}. Analyzing the 1931 \textit{Table} allows us to lay the groundwork for a broader study that will extend across the entire decade, with the aim of capturing the parliamentary activity of the Senate and, subsequently, of the Chamber of Deputies.


%\textbf{Problem:} \textit{[bon là, la 1ère phrase, j'ai un peu riffé... Je ne sais pas si c'est vrai]} Yet, the evaluation of these outputs remains dominated by symbolic “exact match” metrics: Exact Match, Precision/Recall/F-score at the node or parent-child pair level. These metrics are blind to semantic proximity: a predicted label that differs by just one level in the hierarchy or by a lexical variant is penalized as heavily as a completely unrelated prediction, which distorts the measurement of actual performance.

%\joseph{Partie trop technique pour l'intro selon moi, à déplacer dans la partie "Evaluation Framework"}

%\marie{Dans ce cas, il faut supprimer la Research question et enchaîner direct ensuite sur Case study}

%\textbf{Hypothesis:} Optimal Transport (OT) provides a mathematical framework to measure the minimal cost of transforming one distribution into another: here, the distribution of labels generated by the LLM versus the ground truth. By assigning a cost proportional to the semantic/hierarchical distance between labels, OT softens the penalty: the closer the prediction, the lower the cost. The originality of this case study lies in the combination of OT + structured output, which is rarely explored for document indexing tasks. 

%\textbf{Research question:} To what extent does Optimal Transport offer a more fine-grained, human-aligned evaluation than classical metrics when assessing the structured output of an LLM on a document indexing task?

%\textbf{Case study:} Senate tables. Brief presentation of the source and its usefulness.


After reviewing existing approaches to structured data extraction and evaluation (Section~\ref{sec:related-work}), we present three main contributions.
