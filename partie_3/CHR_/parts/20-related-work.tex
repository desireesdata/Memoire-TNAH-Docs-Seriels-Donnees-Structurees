\section{Related Work}
\label{sec:related-work}

Building upon our initial argument, we structure our review around three key questions: 
\begin{inparaenum}[(1)]
    \item How can structured data be effectively modeled? 
    \item How can the quality of structured data produced by such approaches be evaluated? 
    \item How can structured data be generated from text?
\end{inparaenum}

\subsection{Modeling Structured Data}
\label{sec:modeling-structured-data}

Structured data can take various forms, including: 

\textbf{Record Sets:} These are unordered sets of tuples, similar to database tables where columns represent attributes of objects. This structure is commonly employed in Information Extraction tasks, such as named entity recognition~\cite{nadeau_survey_2007} or relation extraction~\cite{mintz_distant_2009}.

\textbf{Record Sequences:} These are ordered versions of record sets, where the sequence of records holds significance. The order may reflect criteria such as time or facilitate tasks like cross-validation, as seen in directories.

\textbf{Trees:} These hierarchical structures are often used to represent nested relationships or dependencies, such as in dependency parsing~\cite{de-marneffe-etal-2014-universal}.

\textbf{Graphs:} These are flexible structures used to represent consolidated knowledge, such as ontologies or knowledge graphs. While these are widely studied, their evaluation typically falls outside the scope of Information Extraction and is beyond the focus of this work.

In this paper, we focus on record sets and sequences, as they are the most relevant for our case study involving the extraction of structured data from parliamentary documents. The various models we experimented with are described in~\Cref{sec:documents-approach}.

\subsection{Evaluating the Quality of Structured Data}
\label{sec:evaluating-structured-data}

The evaluation of structured data quality can be broadly categorized into two types of metrics: edit distance metrics and matching metrics, as defined in~\cite{chen_unified_2023}.

\textbf{Edit Distance Metrics:}  
These metrics involve complex optimization processes and are often computationally intensive. Additionally, their interpretability is limited, as they do not provide a direct comparison between the produced and expected data. Examples include the classical Levenshtein distance for character-level comparisons and Tree-Edit Distance~\cite{zhang_simple_1989} for tree structures. While general graph edit distance metrics exist, their complexity often renders them impractical for real-world applications.

\textbf{Matching Metrics:}  
Matching metrics are more interpretable, as they explicitly identify the elements that match between the produced and expected data. Most approaches rely on bipartite matching between the predicted and reference data sets, computing scores based on the number of matched elements~\cite{chen_unified_2023}. Common metrics include the F1 score, which combines precision and recall, and the Jaccard index, which measures set similarity. However, fewer studies address structured data or partial matching, where the produced data may not perfectly align with the expected data. 
It is interesting to note that the computer vision community share the exact same problem, and a similar framework is proposed in the context of the COCO Panoptic Segmentation Challenge~\cite{kirillov_panoptic_2019}. The evaluation protocol relies on an optimal matching between the surfaces of the predicted and ground truth segmentations to jointly evaluate detection, segmentation and classification of regions ---~which is similar to the approch of Chen et al.~\cite{chen_unified_2023}.

In our work, we adopt a matching metric based on optimal matching between structured data sets, which generalizes the bipartite matching approach while accommodating partial matches. This approach not only provides a quantitative evaluation of data quality but also identifies missed or hallucinated elements, offering actionable insights.

\subsection{Producing Structured Data from Text}
\label{sec:producing-structured-data}

Approaches for generating structured data from text can be broadly divided into two categories: detection-based (extractive) and generation-based (generative or abstractive).

\textbf{Detection-Based Approaches:}  
These methods focus on identifying text fragments corresponding to specific fields or elements of the structured data. Traditional approaches relied on rule-based methods, such as regular expressions or heuristics. More recently, machine learning models, such as sequence labeling models (e.g., CRF-based methods~\cite{finkel_incorporating_2005}) or transformer-based encoder-only models (e.g., BERT~\cite{devlin-etal-2019-bert}), have become prevalent. Transformer-based models are particularly effective due to their intrinsic capabilities and their ability to be fine-tuned on specific tasks with relatively small datasets. Additionally, their design enforces strict alignment between input text and output labels, reducing the risk of hallucinations (i.e., generating data not present in the input). However, these approaches require task-specific training, which demands data, computational resources, expertise, and time.

\textbf{Generation-Based Approaches:}  
These methods leverage autoregressive models to generate structured data by ``translating'' the input text into the desired format. The advent of LLMs has spurred interest in this approach due to their impressive generalization capabilities across diverse tasks without requiring task-specific training~\cite{radford_improving_2018,radford_language_2019,brown_language_2020}. Outputs can be constrained to specific formats, such as JSON, or more complex structures, as long as the set of valid tokens can be dynamically computed~\cite{willard2023efficient}. These models can produce complex, nested structures by generating elements sequentially and can infer implicit elements not explicitly present in the input text. However, their primary drawback is their susceptibility to hallucinations, which are challenging to detect.

In this paper, we explore the effectiveness of generation-based approaches for handling repetitive structures in parliamentary indexes or \textit{Tables}. We leverage the zero-shot capabilities of LLMs and evaluate the viability of this approach for generating structured data in this specific context.

% Note: Hybrid approaches, which combine detection-based and generation-based methods (e.g., using LLMs to generate training data for detection-based models), exist but require significant computational resources and expertise. These approaches are beyond the scope of this work.
