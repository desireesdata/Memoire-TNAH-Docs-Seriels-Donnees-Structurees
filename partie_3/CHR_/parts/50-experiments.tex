\section{Experiments}
\label{sec:experiments}
The evaluation setup was established through an initial development phase, during which the data model, prompt instructions, and reference structured data for a selected development page were iteratively refined. After finalizing this phase, we applied the baseline model to a broader set of pages and manually corrected the outputs to construct an unbiased ground truth for evaluation.

This section details the resulting dataset, the variants generated for analysis, and the evaluation protocol employed to rigorously assess prediction quality.


\subsection{Dataset}
\label{sec:xp-dataset}

% Overview of dataset construction
Ground truth data were constructed to rigorously evaluate extraction quality across varying levels of OCR text fidelity. For each selected page, three distinct OCR variants were generated using the PERO OCR engine~\cite{kis2021atst,kodym2021layout,kohut2021tsnet}:
\begin{inparaenum}[(1)] 
    \item a manually corrected version serving as the gold standard, 
    \item a version with manual layout segmentation, and 
    \item a raw, uncorrected OCR output.
\end{inparaenum}
This design enables systematic assessment of the extraction pipeline’s robustness to noise and layout artifacts.

% Page sampling and annotation protocol
A random sample of five sequential pages was selected for manual transcription and annotation. Because pages are not necessarily contiguous, some speaker entries may be incomplete; in such cases, the LLM was instructed to omit these partial elements. This setup reflects realistic extraction challenges, as entries may span multiple pages. (Handling such cases in production would require multi-page or streaming input, which is beyond the scope of this study.)

% Extraction and evaluation protocol
Each page was processed independently to avoid optimistic bias from sequential context. This sometimes resulted in extractions starting mid-entry, providing the LLM with truncated information and highlighting its ability to interpret partial context. Although the source documents are generally well-digitized, occasional distortions (e.g., page folds) and fragmented input introduce realistic difficulties, exposing model limitations in non-ideal conditions.

% Output generation and schema constraints
Structured outputs were generated using the Ministral 8B model, with a fixed prompt and a Pydantic schema to enforce output consistency. A temperature of zero was used to ensure deterministic results. For each page and OCR variant, the model produced structured JSON outputs, which were then compared to manually curated ground truth representations.

% Summary statistics
The evaluation covers 109 entries across five pages, with each entry assessed for all three OCR conditions.




\subsection{Structured Output Evaluation Protocol}
The evaluation aims to rigorously assess the structured outputs generated by the LLM (denoted as $P$) against a manually curated ground truth ($G$). As described in \Cref{sec:approach-datamodel}, each data instance consists of a \textbf{list of speaker entries}, where each entry comprises a \textbf{speaker name} and a \textbf{list of pages} referencing their speeches.

A key challenge arises from the fact that the model may produce the correct set of entries but in a different order, or with minor structural variations. To address this, we adopt a flexible alignment strategy inspired by prior work~\cite{chen_unified_2023,kirillov_panoptic_2019}, leveraging optimal transport to establish a one-to-one correspondence between predicted and ground truth entries. This approach accommodates order invariance and tolerates minor discrepancies, enabling a robust evaluation of extraction quality.

\subsubsection{Entry-level Distance and Optimal Assignment}
To rigorously compare predicted and ground truth entries, we define a normalized entry-level distance function that quantifies the similarity between each pair.

For the textual component (senator name), we compute the Ratcliff/Obershelp distance, which measures the similarity based on the longest common subsequence, after lowercasing and trimming whitespace. This yields a normalized distance $d_n(g_i, p_j) \in [0, 1]$, where 0 indicates an exact match and 1 indicates complete dissimilarity.

For the list of referenced pages, we use the Intersection-over-Union (IoU) set distance:
$$
d_p(g_i, p_j) = 1 - \frac{|\text{ref\_pages}(g_i) \cap \text{ref\_pages}(p_j)|}{|\text{ref\_pages}(g_i) \cup \text{ref\_pages}(p_j)|}.
$$

The overall entry distance is defined as the product of these two components:
$$
d_e(g_i, p_j) = d_n(g_i, p_j) \times d_p(g_i, p_j).
$$

To establish a one-to-one correspondence between predicted and ground truth entries, we employ optimal transport~\cite{peyré2020computationaloptimaltransport}. This approach finds the assignment that minimizes the total distance across all pairs, accommodating order invariance and structural discrepancies. The resulting alignment provides a principled basis for evaluating extraction quality at the entry level.


\subsubsection{Limitations of Standard Metrics: Precision, Recall, and F1-Score}

Conventional metrics such as \textbf{precision}, \textbf{recall}, and \textbf{F1-score} are widely used to evaluate extraction tasks. Precision quantifies the proportion of correctly generated entries among all model outputs, while recall measures the fraction of ground truth entries successfully retrieved. The F1-score, as their harmonic mean, is intended to provide a balanced summary of performance.

However, in our evaluation protocol—where predicted and ground truth entries are aligned one-to-one using optimal transport—these metrics become unreliable. The injective nature of the alignment ensures that the number of predicted entries always matches the number of ground truth entries. As a result, precision is trivially maximized, regardless of the actual quality of the matches, and recall fails to reflect missing or spurious entries, since all elements are forcibly paired. Consequently, the F1-score inherits these distortions, leading to an inflated and potentially misleading assessment of model performance.

\subsubsection{Integrated Matching Quality (IMQ): A Robust Evaluation Metric}

While standard metrics fail to capture the nuanced quality of structured matches, our protocol leverages the entry-level distance $d_e$ to quantify the fidelity of each aligned pair. For each match, we define a quality score $q_i = 1 - d_e(g_i, p_i)$, where $q_i \in [0, 1]$ reflects the closeness of the predicted entry to its ground truth counterpart.

Rather than imposing an arbitrary threshold to classify matches as correct or incorrect, we adopt an area-under-curve (AUC) approach to aggregate match quality across all possible thresholds. Specifically, for each threshold $t \in [0, 1]$, we compute the proportion of matches with $q_i \geq t$:
$$
F(t) = \frac{1}{n} \sum_{k=1}^{n} \mathbb{1}_{q_k \geq t}
$$
The \textbf{Integrated Matching Quality (IMQ)} is then defined as the area under this curve:
$$
\text{IMQ} = \int_{0}^{1} F(t) \, dt
$$

IMQ provides a comprehensive summary of extraction quality, rewarding both the number and the closeness of matches. A score of 1 indicates perfect alignment, while lower values reflect increasing divergence. This continuous, threshold-free metric is particularly well-suited to evaluating LLM outputs, where minor deviations are common even under strong structural constraints. IMQ thus enables a principled, fine-grained assessment of structured extraction performance.

%%%%%%%%%%%%%%%%%%%%%%% !!! %%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Qualitative Analysis via Matching Scores}
\joseph{only include / rephrase these ideas if we manage to include some qualitative analysis in \Cref{sec:results}!}
The entry-level matching scores produced by our evaluation protocol provide direct insight into extraction errors. Low-quality matches—those with low similarity scores—highlight cases where the model either hallucinated content or failed to extract a ground truth entry. By inspecting these specific alignments, we can efficiently diagnose systematic errors or challenging cases in the extraction process.
\end{comment}
